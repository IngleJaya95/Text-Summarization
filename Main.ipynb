{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sp/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.system(\"export CUDA_VISIBLE_DEVICES=''\")\n",
    "import tensorflow as tf\n",
    "import pickle # for loading and saving the file\n",
    "import numpy as np # for mathematical op in python\n",
    "from tensorflow.contrib.rnn import LSTMCell, LSTMStateTuple\n",
    "from tensorflow.python.layers import core as layers_core\n",
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "import bleu\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "## If not interactive then uncomment it.\n",
    "# import argparse\n",
    "# parser = argparse.ArgumentParser()\n",
    "# parser.add_argument(\"--encoder_hidden_units\",type=int, help=\"The no. of hidden units your encoder should have\",default=512)\n",
    "# parser.add_argument(\"--decoder_hidden_units\",type=int, help=\"The no. of hidden units your decoder should have\",default=512)\n",
    "# parser.add_argument(\"--batch\",type=int, help=\"The no of element in a batch\",default=20)\n",
    "# parser.add_argument(\"--maxEncoderTime\",type=int, help=\"The maximum lenght encoder can have\",default=150)\n",
    "# parser.add_argument(\"--maxDecoderTime\",type=int, help=\"The maximum length decoder can have\",default=150)\n",
    "# parser.add_argument(\"--max_gradient_norm\",type=float, help=\"The clipping limit on the gradient\",default=5.0)\n",
    "# parser.add_argument(\"--lr\",type=float, help=\"Learning Rate\",default=0.0001)\n",
    "# parser.add_argument(\"--epoch_1\",type=int, help=\"Training Epochs using Training Helper\",default=4)\n",
    "# parser.add_argument(\"--epoch_2\",type=int, help=\"Training Epochs using Greedy Helper\",default=20)\n",
    "# parser.add_argument(\"--val_step_shift\",type=int, help=\"After how many step you wish to have a log entry in your log file\",default=100)\n",
    "# parser.add_argument(\"--beamWidth\",type = int, help=\"Set the beam width\",default=0)\n",
    "# parser.add_argument(\"--beamSearch\", type= bool, help=\"True : beamSearch enable, False : BeamSearch disable\",default=False)\n",
    "# parser.add_argument(\"--con_name\",type=str ,help=\"The name that make this experiment unique\",default=\"Awesome_hehe\")\n",
    "# parser.add_argument(\"--encoder_choice\",type=int,help=\"0: Bi-directional Encoder, 1: Unidirectional Encoder\",default=0)\n",
    "# parser.add_argument(\"--mode\",type=int,help=\"0: basic Decoder, 1: basic decoder with attention, 2: hirarchial decoder\",default=0)\n",
    "# parser.add_argument(\"--storingThreshold\",type=float,help=\"The thresold bleu score after which we prefer to store\",default=0.4)\n",
    "# parser.add_argument(\"--dropout\",type=int,help=\"1: To apply dropout, 0: No dropout\",default=0)\n",
    "# parser.add_argument(\"--dropval\",type=float,help=\"Give the probability to drop\",default=0.4)\n",
    "# args = parser.parse_args()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ##############################################################\n",
    "# ############# Hyper Parameter Section ########################\n",
    "# ##############################################################\n",
    "# #1\n",
    "# encoder_hidden_units = args.encoder_hidden_units\n",
    "# #2\n",
    "# decoder_hidden_units = args.decoder_hidden_units\n",
    "\n",
    "# #3\n",
    "# batchSize = args.batch#20\n",
    "# #4\n",
    "# maxEncoderTime = args.maxEncoderTime #150\n",
    "# #5\n",
    "# maxDecoderTime = args.maxDecoderTime #150\n",
    "\n",
    "# # The maximum value a gradient can achieve. If more will be clipped\n",
    "# #6\n",
    "# max_gradient_norm = args.max_gradient_norm #5\n",
    "\n",
    "# # The Learning Rate\n",
    "# #6\n",
    "# learning_rate = args.lr #0.0001\n",
    "\n",
    "# #The Epochs : epoch_1 corrosponds to training using training Helper.\n",
    "# # epoch_2 corrosponds to training using greedy helper\n",
    "# #8\n",
    "# epoch_1 =  args.epoch_1 #2\n",
    "# #9\n",
    "# epoch_2 = args.epoch_2 #2\n",
    "\n",
    "# epochs = epoch_1 + epoch_2\n",
    "\n",
    "# #10\n",
    "# val_step_shift = args.val_step_shift #100 # After how many step you wish to have a log\n",
    "# sno = 0 # Just to make identification easier\n",
    "# #On which epoch wish to switch from trainHelper to greedy helper\n",
    "\n",
    "# # If bleu score is greater than the threshold then save the model\n",
    "# #11\n",
    "# storingThreshold = args.storingThreshold #0.4\n",
    "\n",
    "# # For the sake of beam Search\n",
    "# #11\n",
    "# beamWidth = args.beamWidth  #10\n",
    "# #12\n",
    "# beamSearch =args.beamSearch #False\n",
    "\n",
    "# # The name which respective indicative file will have in their respective directory\n",
    "# #13\n",
    "# con_name = args.con_name #\"Trying\"\n",
    "\n",
    "# # Bidirectional or Unidirectional\n",
    "# # 0: for the bidirectional\n",
    "# # 1: for the unidirectional\n",
    "# #14\n",
    "# encoder_choice = args.encoder_choice #0\n",
    "\n",
    "\n",
    "# attenType = \"Dhoka\"\n",
    "# # Validation size of the Batch\n",
    "# val_batch_size = batchSize\n",
    "\n",
    "# #mode =0 basic ,1 basic with attention ,hiearchical = 2 \n",
    "# #15\n",
    "# mode =  args.mode # 0\n",
    "\n",
    "# dropoutApplier = args.dropout\n",
    "# dropval = args.dropval\n",
    "# dpFixed = dropval\n",
    "##############################################################################################\n",
    "\n",
    "\n",
    "##############################################################\n",
    "############# Hyper Parameter Section ########################\n",
    "##############################################################\n",
    "encoder_hidden_units = 1024\n",
    "decoder_hidden_units = 1024\n",
    "\n",
    "batchSize = 50\n",
    "maxEncoderTime = 150\n",
    "maxDecoderTime = 150\n",
    "\n",
    "dropoutApplier = 1\n",
    "dropval = 0.8\n",
    "dpFixed = dropval\n",
    "# The maximum value a gradient can achieve. If more will be clipped\n",
    "max_gradient_norm = 5\n",
    "\n",
    "# The Learning Rate\n",
    "learning_rate = 0.0001\n",
    "\n",
    "#The Epochs : epoch_1 corrosponds to training using training Helper.\n",
    "# epoch_2 corrosponds to training using greedy helper\n",
    "epoch_1 = 1\n",
    "epoch_2 = 0\n",
    "\n",
    "epochs = epoch_1 + epoch_2\n",
    "val_step_shift = 100 # After how many step you wish to have a log\n",
    "sno = 0 # Just to make identification easier\n",
    "#On which epoch wish to switch from trainHelper to greedy helper\n",
    "\n",
    "# If bleu score is greater than the threshold then save the model\n",
    "storingThreshold = 0.4\n",
    "\n",
    "# For the sake of beam Search\n",
    "beamWidth = 10\n",
    "beamSearch = False\n",
    "\n",
    "# The name which respective indicative file will have in their respective directory\n",
    "con_name = \"Trying\"\n",
    "\n",
    "# Bidirectional or Unidirectional\n",
    "# 0: for the bidirectional\n",
    "# 1: for the unidirectional\n",
    "encoder_choice = 1\n",
    "\n",
    "\n",
    "attenType = \"Dhoka\"\n",
    "# Validation size of the Batch\n",
    "val_batch_size = batchSize\n",
    "\n",
    "#mode =0 basic ,1 basic with attention ,hiearchical = 2 \n",
    "mode = 1\n",
    "\n",
    "#################################################################\n",
    "################# Hyper Parameter Section ends here #############\n",
    "#################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################################################\n",
    "############ Support Code Section ###############################\n",
    "#################################################################\n",
    "\n",
    "# How to use : create a folder name \"repo\" in the directory\n",
    "# where file is located\n",
    "# obj : the object required to save\n",
    "# name : the name for the host file of the data\n",
    "\n",
    "def saveObject(obj,name): \n",
    "    pickle.dump(obj,open( \"repo/data/core/\"+name+\".pkl\", \"wb\" ))\n",
    "\n",
    "def loadObject(name):\n",
    "    obj = pickle.load( open( \"repo/data/core/\"+name+\".pkl\", \"rb\" ) )\n",
    "    return obj\n",
    "\n",
    "fileName = \"repo/log/\" +con_name+ str(sno) + \"_Mode_\" + str(mode) + \"_batchSize_\" + str(batchSize) +\"_atype_\" +attenType +\"_epochs_\" + str(epochs) + \".csv\"\n",
    "logFile = open(fileName,\"w\")\n",
    "heading = \"epoch ,steps ,train_loss ,val_loss ,bleu_score \\n\"\n",
    "logFile.write(heading)\n",
    "logFile.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################################\n",
    "########### Function Important ###############################\n",
    "##############################################################\n",
    "\n",
    "def enc_data_gen(file , pad_num):\n",
    "    data = file.read()\n",
    "    spl = data.split(\"\\n\")\n",
    "    del spl[len(spl)-1]    # becoz last line is ''\n",
    "    \n",
    "    dat_mat = np.zeros([len(spl),150]) + pad_num        \n",
    "    i = 0\n",
    "    len_cont = [] # to contain the lengths of sequence\n",
    "    for line in spl:\n",
    "        textstr = line.split(' ')\n",
    "        del textstr[len(textstr)-1]\n",
    "        newLine = []\n",
    "        for tex in textstr:\n",
    "            if tex not in encT2N.keys():\n",
    "                tex = '<unk>'\n",
    "            textVal = encT2N[tex]\n",
    "            newLine.append(textVal)\n",
    "        length = len(newLine)\n",
    "        dat_mat[i,:length] = np.array(newLine)\n",
    "        len_cont.append(length) \n",
    "        i=i+1\n",
    "\n",
    "# just for a better/powerful future \n",
    "    len_cont = np.array(len_cont)\n",
    "    return dat_mat,len_cont\n",
    "\n",
    "def dec_inp_gen(file , pad_num):\n",
    "    data = file.read()\n",
    "    spl = data.split(\"\\n\")\n",
    "    del spl[len(spl)-1]    # becoz last line is ''\n",
    "    \n",
    "    dat_mat = np.zeros([len(spl),150]) +pad_num        \n",
    "    i = 0\n",
    "    len_cont = [] # to contain the lengths of sequence\n",
    "    for line in spl:\n",
    "        textstr = line.split(' ')\n",
    "        del textstr[len(textstr)-1]\n",
    "        newLine = []\n",
    "        for tex in textstr:\n",
    "            if tex not in outT2N.keys():\n",
    "                tex = '<unk>'\n",
    "            textVal = outT2N[tex]\n",
    "            newLine.append(textVal)\n",
    "        length = len(newLine)\n",
    "        #It has to start from <GO>.\n",
    "        dat_mat[i,0] = outT2N['<GO>']        \n",
    "        dat_mat[i,1:length+1]= np.array(newLine)\n",
    "        len_cont.append(length+1) \n",
    "        i=i+1\n",
    "\n",
    "    # just for a better future \n",
    "    len_cont = np.array(len_cont)\n",
    "    return dat_mat,len_cont\n",
    "\n",
    "def dec_out_gen(file , pad_num):\n",
    "    data = file.read()\n",
    "    spl = data.split(\"\\n\")\n",
    "    del spl[len(spl)-1]    # becoz last line is ''\n",
    "    \n",
    "    dat_mat = np.zeros([len(spl),150]) +pad_num        \n",
    "    i = 0\n",
    "    len_cont = [] # to contain the lengths of sequence\n",
    "    for line in spl:\n",
    "        textstr = line.split(' ')\n",
    "        del textstr[len(textstr)-1]\n",
    "        newLine = []\n",
    "        for tex in textstr:\n",
    "            if tex not in outT2N.keys():\n",
    "                tex = '<unk>'\n",
    "            textVal = outT2N[tex]\n",
    "            newLine.append(textVal)\n",
    "        length = len(newLine)\n",
    "        dat_mat[i,:length]= np.array(newLine)\n",
    "        #Since it has to end with 'End of String'\n",
    "        dat_mat[i,length] = outT2N['<EOS>']\n",
    "        len_cont.append(length+1)\n",
    "        i=i+1\n",
    "\n",
    "    # just for a better future \n",
    "    len_cont = np.array(len_cont)\n",
    "    return dat_mat,len_cont\n",
    "\n",
    "def target_w_gen(file):\n",
    "    data = file.read()\n",
    "    spl = data.split(\"\\n\")\n",
    "    del spl[len(spl)-1]    # becoz last line is ''\n",
    "    \n",
    "    dat_mat = np.zeros([len(spl),150])        \n",
    "    i = 0\n",
    "    len_cont = [] # to contain the lengths of sequence\n",
    "    for line in spl:\n",
    "        textstr = line.split(' ')\n",
    "        del textstr[len(textstr)-1]\n",
    "        length = len(textstr)\n",
    "        dat_mat[i,:length+1]= 1\n",
    "        #Since it has to end with 'End of String'\n",
    "        i=i+1\n",
    "    # just for a better future \n",
    "    return dat_mat\n",
    "\n",
    "def val_belu_ref_gen(file):\n",
    "    data = file.read()\n",
    "    spl = data.split(\"\\n\")\n",
    "    del spl[len(spl)-1]    # becoz last line is ''\n",
    "    texColl = [] # Contain All the string in tokken format\n",
    "    for line in spl:\n",
    "        textstr = line.split(' ')\n",
    "        del textstr[len(textstr)-1]\n",
    "        texColl.append(textstr)\n",
    "    # just for a better future \n",
    "    return texColl\n",
    "\n",
    "\n",
    "\n",
    "# The code that generate the data\n",
    "def Datafeeder(step,batch_size,trainMet=True):\n",
    "    #encSequenceLen feeder\n",
    "    feedEncSeqLen = train_enc_inp_len[step*batch_size:(step+1)*batch_size]\n",
    "    #decSequenceLen feeder\n",
    "    feedDecSeqLen = train_dec_inp_len[step*batch_size:(step+1)*batch_size]\n",
    "\n",
    "    enc_truncate_factor = max(feedEncSeqLen)\n",
    "    dec_truncate_factor = max(feedDecSeqLen)\n",
    "\n",
    "    #Encoder Input feeder\n",
    "    enc_fd = train_enc_inp_mat[step*batch_size:(step+1)*batch_size]\n",
    "    enc_tfd = enc_fd[:,:enc_truncate_factor]\n",
    "    feedEncInput = (enc_tfd).T # Transpose since Time major format\n",
    "\n",
    "    #Decoder Input feeder\n",
    "    dec_fd = train_dec_inp_mat[step*batch_size:(step+1)*batch_size]\n",
    "    dec_tfd = dec_fd[:,:dec_truncate_factor]\n",
    "    feedDecInput = (dec_tfd).T\n",
    "\n",
    "    #Decoder Output Feeder\n",
    "    dec_fd = train_dec_out_mat[step*batch_size:(step+1)*batch_size]\n",
    "    dec_tfd = dec_fd[:,:dec_truncate_factor]\n",
    "    feedDecOutput = (dec_tfd).T\n",
    "\n",
    "    #TargetWeights Feeder\n",
    "    dec_fd = train_target_w_mat[step*batch_size:(step+1)*batch_size]\n",
    "    dec_tfd = dec_fd[:,:dec_truncate_factor]\n",
    "    feedTargetW = (dec_tfd).T\n",
    "\n",
    "    inputData = {encoder_inputs:feedEncInput,decoder_inputs:feedDecInput,decoder_outputs:feedDecOutput,\n",
    "             target_weights:feedTargetW,enc_seqLen:feedEncSeqLen,dec_seqLen:feedDecSeqLen,choice:trainMet,\n",
    "                max_dec_len : dec_truncate_factor}\n",
    "    \n",
    "    return inputData\n",
    "\n",
    "\n",
    "\n",
    "def val_Datafeeder(step,batch_size,trainMet=True):\n",
    "    #encSequenceLen feeder\n",
    "    feedEncSeqLen = val_enc_inp_len[step*batch_size:(step+1)*batch_size]\n",
    "    #decSequenceLen feeder\n",
    "    feedDecSeqLen = val_dec_inp_len[step*batch_size:(step+1)*batch_size]\n",
    "\n",
    "    enc_truncate_factor = max(feedEncSeqLen)\n",
    "    dec_truncate_factor = max(feedDecSeqLen)\n",
    "\n",
    "    #Encoder Input feeder\n",
    "    enc_fd = val_enc_inp_mat[step*batch_size:(step+1)*batch_size]\n",
    "    enc_tfd = enc_fd[:,:enc_truncate_factor]\n",
    "    feedEncInput = (enc_tfd).T # Transpose since Time major format\n",
    "\n",
    "    #Decoder Input feeder\n",
    "    dec_fd = val_dec_inp_mat[step*batch_size:(step+1)*batch_size]\n",
    "    dec_tfd = dec_fd[:,:dec_truncate_factor]\n",
    "    feedDecInput = (dec_tfd).T\n",
    "\n",
    "    #Decoder Output Feeder\n",
    "    dec_fd = val_dec_out_mat[step*batch_size:(step+1)*batch_size]\n",
    "    dec_tfd = dec_fd[:,:dec_truncate_factor]\n",
    "    feedDecOutput = (dec_tfd).T\n",
    "\n",
    "    #TargetWeights Feeder\n",
    "    dec_fd = val_target_w_mat[step*batch_size:(step+1)*batch_size]\n",
    "    dec_tfd = dec_fd[:,:dec_truncate_factor]\n",
    "    feedTargetW = (dec_tfd).T\n",
    "\n",
    "    inputData = {encoder_inputs:feedEncInput,decoder_inputs:feedDecInput,decoder_outputs:feedDecOutput,\n",
    "             target_weights:feedTargetW,enc_seqLen:feedEncSeqLen,dec_seqLen:feedDecSeqLen,choice:trainMet,\n",
    "                max_dec_len : dec_truncate_factor}\n",
    "    \n",
    "    return inputData\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def test_Datafeeder(step,batch_size,trainMet=True):\n",
    "    #encSequenceLen feeder\n",
    "    feedEncSeqLen = test_enc_inp_len_enp[step*batch_size:(step+1)*batch_size]\n",
    "    #decSequenceLen feeder\n",
    "    feedDecSeqLen = test_enc_inp_len_enp[step*batch_size:(step+1)*batch_size]\n",
    "\n",
    "    enc_truncate_factor = int(max(feedEncSeqLen))\n",
    "    dec_truncate_factor = int(max(feedDecSeqLen))\n",
    "\n",
    "    #Encoder Input feeder\n",
    "    enc_fd = test_enc_inp_mat_enp[step*batch_size:(step+1)*batch_size]\n",
    "    enc_tfd = enc_fd[:,:enc_truncate_factor]\n",
    "    feedEncInput = (enc_tfd).T # Transpose since Time major format\n",
    "\n",
    "    #Decoder Input feeder\n",
    "    dec_fd = test_enc_inp_mat_enp[step*batch_size:(step+1)*batch_size]\n",
    "    dec_tfd = dec_fd[:,:dec_truncate_factor]\n",
    "    feedDecInput = (dec_tfd).T\n",
    "\n",
    "    #Decoder Output Feeder\n",
    "    dec_fd = test_enc_inp_mat_enp[step*batch_size:(step+1)*batch_size]\n",
    "    dec_tfd = dec_fd[:,:dec_truncate_factor]\n",
    "    feedDecOutput = (dec_tfd).T\n",
    "\n",
    "    #TargetWeights Feeder\n",
    "    dec_fd = test_enc_inp_mat_enp[step*batch_size:(step+1)*batch_size]\n",
    "    dec_tfd = dec_fd[:,:dec_truncate_factor]\n",
    "    feedTargetW = (dec_tfd).T\n",
    "\n",
    "    inputData = {encoder_inputs:feedEncInput,decoder_inputs:feedDecInput,decoder_outputs:feedDecOutput,\n",
    "             target_weights:feedTargetW,enc_seqLen:feedEncSeqLen,dec_seqLen:feedDecSeqLen,choice:trainMet,\n",
    "                max_dec_len : dec_truncate_factor}\n",
    "    \n",
    "    return inputData\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# For Generating Text for a batch\n",
    "def GenerateText(genText):\n",
    "    genText = np.array(genText)\n",
    "    # MaxTime * BatchSize is for systems, I love the transpose.\n",
    "    # Standard way to represent the text\n",
    "    genText = genText.T\n",
    "    lines,maxWords = np.shape(genText)\n",
    "\n",
    "    collectionText = []\n",
    "\n",
    "    for i in range(lines):\n",
    "        line_with_no = genText[i]\n",
    "        line_text = []\n",
    "        for word_no in line_with_no:\n",
    "            word_text = outN2T[word_no]\n",
    "            if word_text != \"<PAD>\" and word_text != \"<unk>\":\n",
    "                if word_text == \"<EOS>\":\n",
    "                    break\n",
    "                line_text.append(word_text)\n",
    "        l_text = \" \".join(line_text)\n",
    "        collectionText.append(l_text)\n",
    "    return collectionText\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "#Creating PADDING, EOS and GO\n",
    "\n",
    "key1 = '<PAD>'\n",
    "key2 = '<EOS>'\n",
    "key3 = '<GO>'\n",
    "\n",
    "val1 = np.array([np.zeros(256)])\n",
    "val2 = np.array([np.zeros(256)-0.5])\n",
    "val3 = np.array([np.zeros(256)+0.5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "inEmb = loadObject(\"inpEmb\")\n",
    "# inEmb is a dictionary \n",
    "# that contain the input embedding : embedding\n",
    "# A text to no disctionary : encT2N\n",
    "# A no to text disctionary : encN2T\n",
    "\n",
    "encN2T = inEmb['encT2N']\n",
    "encT2N = inEmb['encN2T']\n",
    "embInp = inEmb['embedding']\n",
    "\n",
    "Len = len(encN2T)\n",
    "encN2T[Len] = key1\n",
    "encN2T[Len+1] = key2\n",
    "encN2T[Len+2] = key3\n",
    "encT2N[key1] = Len\n",
    "encT2N[key2] = Len + 1\n",
    "encT2N[key3] = Len + 2\n",
    "embInp = np.append(embInp,val1,0)\n",
    "embInp = np.append(embInp,val2,0)\n",
    "embInp = np.append(embInp,val3,0)\n",
    "enc_vocab_size = len(encT2N)\n",
    "\n",
    "#Embedding Exchanged\n",
    "currentIndex = encT2N[\"<PAD>\"] \n",
    "targetIndex = 0\n",
    "\n",
    "currentKey = \"<PAD>\"\n",
    "targetKey = encN2T[targetIndex]\n",
    "\n",
    "#Exchange The embeddings\n",
    "tempEmb = np.zeros([1,256])\n",
    "tempEmb[0,:] = embInp[currentIndex,:]\n",
    "embInp[currentIndex,:] = embInp[targetIndex,:]\n",
    "embInp[targetIndex,:] = tempEmb[0,:]\n",
    "\n",
    "#Exchange keys and Value\n",
    "encT2N[targetKey] = currentIndex\n",
    "encT2N[currentKey] = targetIndex\n",
    "\n",
    "encN2T[currentIndex] = targetKey\n",
    "encN2T[targetIndex] = currentKey"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "outEmb = loadObject(\"outEmb\")\n",
    "# outEmb is a dictionary \n",
    "# that contain the input embedding : embedding\n",
    "# A text to no disctionary : encT2N\n",
    "# A no to text disctionary : encN2T\n",
    "\n",
    "outN2T = outEmb['outT2N']\n",
    "outT2N = outEmb['outN2T']\n",
    "embOut = outEmb['embedding']\n",
    "\n",
    "Len = len(outN2T)\n",
    "outN2T[Len] = key1\n",
    "outN2T[Len+1] = key2\n",
    "outN2T[Len+2] = key3\n",
    "outT2N[key1] = Len\n",
    "outT2N[key2] = Len + 1\n",
    "outT2N[key3] = Len + 2\n",
    "embOut = np.append(embOut,val1,0)\n",
    "embOut = np.append(embOut,val2,0)\n",
    "embOut = np.append(embOut,val3,0)\n",
    "\n",
    "#Fixing Padding Issue\n",
    "currentIndex = outT2N[\"<PAD>\"]\n",
    "targetIndex = 0\n",
    "\n",
    "currentKey = '<PAD>'\n",
    "targetKey = outN2T[0]\n",
    "\n",
    "#Fixing the embedding matrix\n",
    "temp = np.zeros([1,256])\n",
    "temp[0,:] = embOut[currentIndex,:]\n",
    "embOut[currentIndex,:] = embOut[targetIndex,:]\n",
    "embOut[targetIndex,:] = temp[0,:]\n",
    "\n",
    "#Fixing the dictionary\n",
    "outT2N[currentKey] = targetIndex\n",
    "outT2N[targetKey] = currentIndex\n",
    "\n",
    "outN2T[targetIndex] = currentKey\n",
    "outT2N[currentIndex] = targetKey\n",
    "\n",
    "dec_vocab_size = len(outN2T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reading the Data\n",
    "\n",
    "DataLoc = 'repo/data/'\n",
    "\n",
    "#TrainData\n",
    "train_path = DataLoc + 'train/'\n",
    "train_summariesData = open(train_path+'summaries.txt','r')\n",
    "train_tableData = open(train_path+'train.combined','r')\n",
    "\n",
    "#Formatting Training Data\n",
    "train_enc_inp_mat,train_enc_inp_len=enc_data_gen(train_tableData, encT2N['<PAD>'])\n",
    "train_dec_inp_mat,train_dec_inp_len=dec_inp_gen(train_summariesData, outT2N['<PAD>'])\n",
    "\n",
    "train_summariesData = open(train_path+'summaries.txt','r')\n",
    "train_dec_out_mat,train_dec_out_len=dec_out_gen(train_summariesData, outT2N['<PAD>'])\n",
    "\n",
    "train_summariesData = open(train_path+'summaries.txt','r')\n",
    "train_target_w_mat = target_w_gen(train_summariesData)\n",
    "\n",
    "#ValidationData\n",
    "val_path = DataLoc + 'dev/'\n",
    "\n",
    "# Formatting Validation Data in the required Form\n",
    "val_tableData = open(val_path+'dev.combined','r')\n",
    "val_enc_inp_mat,val_enc_inp_len=enc_data_gen(val_tableData, encT2N['<PAD>'])\n",
    "\n",
    "val_summariesData = open(val_path+'summaries.txt','r')\n",
    "val_dec_inp_mat,val_dec_inp_len=dec_inp_gen(val_summariesData, outT2N['<PAD>'])\n",
    "\n",
    "val_summariesData = open(val_path+'summaries.txt','r')\n",
    "val_dec_out_mat,val_dec_out_len=dec_out_gen(val_summariesData, outT2N['<PAD>'])\n",
    "\n",
    "val_summariesData = open(val_path+'summaries.txt','r')\n",
    "val_target_w_mat = target_w_gen(val_summariesData)\n",
    "\n",
    "val_summariesData = open(val_path+'summaries.txt','r') # For creating reference for BLEU Score\n",
    "val_belu_ref_text = val_belu_ref_gen(val_summariesData)\n",
    "\n",
    "# #Formating Validation Data\n",
    "# valInp_data_mat,valInp_len=data_gen(val_tableData, encT2N['<PAD>'])\n",
    "# valTarget_data_mat,valTarget_len=data_gen(val_summariesData, outT2N['<PAD>'])\n",
    "\n",
    "#TestingData\n",
    "test_path = DataLoc + 'test/'\n",
    "test_tableData = open(test_path+'test.combined','r')\n",
    "\n",
    "#Formatting Test Data\n",
    "test_enc_inp_mat,test_enc_inp_len=enc_data_gen(test_tableData, encT2N['<PAD>'])\n",
    "\n",
    "test_enc_inp_mat_enp = np.zeros([4000,150])\n",
    "len_testing,w_testing = np.shape(test_enc_inp_mat)\n",
    "test_enc_inp_mat_enp[:len_testing,:] = test_enc_inp_mat[:,:]\n",
    "\n",
    "test_enc_inp_len_enp = np.zeros(4000) + 150\n",
    "test_enc_inp_len_enp[:len_testing] = test_enc_inp_len\n",
    "\n",
    "\n",
    "train_dp_count = len(train_enc_inp_len)\n",
    "val_dp_count = len(val_dec_inp_len)\n",
    "test_act_count = len(test_enc_inp_len)\n",
    "test_dp_count = len(test_enc_inp_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'encoder_inputs:0' shape=(?, 50) dtype=int32>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "########################################################################\n",
    "################### Graph #############################################\n",
    "#######################################################################\n",
    "dropoutApplier = 1\n",
    "dropval = 0.8\n",
    "#Reseting the graph\n",
    "tf.reset_default_graph()\n",
    "tf.set_random_seed(1234)\n",
    "\n",
    "#Loading Embedding Matrix\n",
    "embeddingMatrixOut = tf.Variable(embOut, dtype=tf.float32,name=\"embeddingMatrixOut\")\n",
    "embeddingMatrixInp = tf.Variable(embInp, dtype=tf.float32,name=\"embeddingMatrixInp\")\n",
    "\n",
    "#Placeholders\n",
    "encoder_inputs = tf.placeholder(shape=(None,batchSize),dtype=tf.int32,name=\"encoder_inputs\")\n",
    "decoder_inputs = tf.placeholder(shape=(None,batchSize),dtype=tf.int32,name=\"decoder_inputs\")\n",
    "decoder_outputs = tf.placeholder(shape=(None,batchSize),dtype=tf.int32,name=\"decoder_outputs\")\n",
    "\n",
    "\n",
    "target_weights = tf.placeholder(shape=(None,batchSize),dtype=tf.float32,name=\"target_weights\")\n",
    "\n",
    "enc_seqLen = tf.placeholder(shape=(batchSize),dtype=tf.int32,name=\"enc_seqLen\")\n",
    "dec_seqLen = tf.placeholder(shape=(batchSize),dtype=tf.int32,name=\"dec_seqLen\")\n",
    "\n",
    "#Embedding Lookups\n",
    "enc_embedded_input = tf.nn.embedding_lookup(embeddingMatrixInp, encoder_inputs)\n",
    "dec_embedded_input = tf.nn.embedding_lookup(embeddingMatrixOut, decoder_inputs)\n",
    "\n",
    "##############################################################\n",
    "############### Encoder ######################################\n",
    "#############################################################3#\n",
    "\n",
    "#Creating Cell for the Encoder\n",
    "\n",
    "#Encoder Choice 0: for the bidirectional, 1: for the unidirectional\n",
    "\n",
    "if encoder_choice == 0:\n",
    "    #Calling Dynamic RNN\n",
    "    enc_forward_cell = LSTMCell(encoder_hidden_units)\n",
    "    enc_backward_cell = LSTMCell(encoder_hidden_units)\n",
    "    if dropoutApplier == 1:\n",
    "        enc_forward_cell = tf.contrib.rnn.DropoutWrapper(\n",
    "            enc_forward_cell, output_keep_prob=1-dropval)\n",
    "        enc_backward_cell = tf.contrib.rnn.DropoutWrapper(\n",
    "            enc_backward_cell, output_keep_prob=1-dropval)\n",
    "    ((encoder_fw_outputs,\n",
    "      encoder_bw_outputs),\n",
    "     (encoder_fw_final_state,\n",
    "      encoder_bw_final_state)) = (\n",
    "        tf.nn.bidirectional_dynamic_rnn(cell_fw = enc_forward_cell,\n",
    "                                        cell_bw = enc_backward_cell,\n",
    "                                        inputs = enc_embedded_input,\n",
    "                                        sequence_length = enc_seqLen,\n",
    "                                        dtype = tf.float32,\n",
    "                                        time_major = True)\n",
    "        )\n",
    "\n",
    "    #Concatatenating Forward and Backward output\n",
    "    encoder_outputs = tf.concat((encoder_fw_outputs, encoder_bw_outputs), 2)\n",
    "\n",
    "\n",
    "    encoder_final_state_c = tf.concat(\n",
    "        (encoder_fw_final_state.c, encoder_bw_final_state.c), 1)\n",
    "\n",
    "    encoder_final_state_h = tf.concat(\n",
    "        (encoder_fw_final_state.h, encoder_bw_final_state.h), 1)\n",
    "\n",
    "    encoder_final_state = LSTMStateTuple(\n",
    "        c=encoder_final_state_c,\n",
    "        h=encoder_final_state_h\n",
    "    )\n",
    "\n",
    "else:\n",
    "    encoder_cell = tf.nn.rnn_cell.LSTMCell(encoder_hidden_units)\n",
    "    if dropoutApplier == 1:\n",
    "        encoder_cell = tf.contrib.rnn.DropoutWrapper(\n",
    "            encoder_cell, output_keep_prob=1-dropval)\n",
    "    initial_state = encoder_cell.zero_state(batchSize,dtype =tf.float32)\n",
    "    encoder_outputs, encoder_final_state = tf.nn.dynamic_rnn(\n",
    "    encoder_cell, enc_embedded_input, initial_state = initial_state,dtype = tf.float32, \n",
    "    sequence_length=enc_seqLen, time_major=True)\n",
    "    \n",
    "# convW = tf.Variable(tf.random_uniform([2,2*encoder_hidden_units, decoder_hidden_units], -1, 1), dtype=tf.float32,name=\"convW\")\n",
    "# convb = tf.Variable(tf.zeros([decoder_hidden_units]), dtype=tf.float32,name=\"convb\")\n",
    "# #convAct = tf.add(tf.matmul(encoder_hidden_units, convW), convb)\n",
    "# #initDecoder = tf.tanh(convAct)\n",
    "# tf.matmul(convW,encoder_final_state)\n",
    "\n",
    "# Decoder\n",
    "if mode == 1:\n",
    "    attention_states = tf.transpose(encoder_outputs,[1,0,2])\n",
    "    attention_mechanism = tf.contrib.seq2seq.LuongAttention(decoder_hidden_units,attention_states,memory_sequence_length=enc_seqLen)\n",
    "    \n",
    "    decoder_cell = LSTMCell(decoder_hidden_units)\n",
    "    decoder_cell = tf.contrib.seq2seq.AttentionWrapper(decoder_cell,attention_mechanism,attention_layer_size = 2*decoder_hidden_units)\n",
    "    encoder_final_state = decoder_cell.zero_state(dtype =tf.float32,batch_size = batchSize) \n",
    "else:\n",
    "    if encoder_choice == 0:\n",
    "        decoder_cell = LSTMCell(2*decoder_hidden_units)\n",
    "    else:\n",
    "        decoder_cell = LSTMCell(decoder_hidden_units)\n",
    "\n",
    "\n",
    "projection_layer = layers_core.Dense(dec_vocab_size, use_bias=False, name=\"output_projection\")\n",
    "\n",
    "# Attention Is All You Need\n",
    "\n",
    "\n",
    "\n",
    "#Training Helper\n",
    "helper_1 = tf.contrib.seq2seq.TrainingHelper(inputs = dec_embedded_input,sequence_length = dec_seqLen,time_major=True)\n",
    "#Normal Decoder\n",
    "\n",
    "\n",
    "decoder_1 = tf.contrib.seq2seq.BasicDecoder(decoder_cell,helper_1,encoder_final_state,output_layer = projection_layer)\n",
    "#Dynamic Decoding\n",
    "outputs_1, final_context_state_1, _= tf.contrib.seq2seq.dynamic_decode(decoder_1,swap_memory=True,output_time_major=True)\n",
    "logits_1 = outputs_1.rnn_output\n",
    "#Calculating Cross Entropy\n",
    "crossent_1 = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=decoder_outputs,logits = logits_1)\n",
    "#Calculation the loss_1 the so called Train Helper Loss\n",
    "loss_1 = (tf.reduce_sum(crossent_1*target_weights)/batchSize)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Greedy Helper\n",
    "max_dec_len = tf.placeholder(tf.int32,shape=(), name=\"max_dec_len\")\n",
    "\n",
    "tgt_sos_id = outT2N['<GO>']\n",
    "tgt_eos_id = outT2N['<EOS>']\n",
    "\n",
    "helper_2 = tf.contrib.seq2seq.GreedyEmbeddingHelper(\n",
    "    embeddingMatrixOut,\n",
    "    tf.fill([batchSize],tgt_sos_id),tgt_eos_id)\n",
    "\n",
    "#Inferential Decoder\n",
    "inference_decoder = tf.contrib.seq2seq.BasicDecoder(\n",
    "    decoder_cell,\n",
    "    helper_2,\n",
    "    encoder_final_state,\n",
    "    output_layer=projection_layer)\n",
    "\n",
    "#Dynamic Decoder\n",
    "outputs_2, final_context_state_infer_2, _= tf.contrib.seq2seq.dynamic_decode(\n",
    "    inference_decoder,\n",
    "    maximum_iterations=max_dec_len,\n",
    "    output_time_major=True)\n",
    "\n",
    "translations = outputs_2.sample_id\n",
    "#Calculating Logits for greedy helper\n",
    "\n",
    "####\n",
    "\n",
    "\n",
    "logits_2 = outputs_2.rnn_output\n",
    "\n",
    "#Adding Padding to the logits\n",
    "logit_shape = tf.shape(logits_2)\n",
    "rem = max_dec_len - logit_shape[0]\n",
    "paddings = [[0, rem],[0,0],[0,0]]\n",
    "logits_infer = tf.pad(logits_2,paddings,'CONSTANT')\n",
    "\n",
    "# Calculating the cross Entropy\n",
    "crossent_2 = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=decoder_outputs,logits = logits_infer)\n",
    "#Calculating the loss for greedy Helper type\n",
    "loss_2 = (tf.reduce_sum(crossent_2*target_weights)/batchSize)\n",
    "\n",
    "choice = tf.placeholder(dtype=tf.bool)\n",
    "\n",
    "#NotHere \n",
    "################################################\n",
    "#Beamer Search\n",
    "\n",
    "# decoder_initial_state = tf.contrib.seq2seq.tile_batch(\n",
    "# encoder_final_state,\n",
    "# multiplier = 10,\n",
    "# )\n",
    "\n",
    "# decoder_3 = tf.contrib.seq2seq.BeamSearchDecoder(\n",
    "# cell = decoder_cell,\n",
    "# embedding = embeddingMatrixOut,\n",
    "# start_tokens = tgt_sos_id,\n",
    "# end_tokens = tgt_eos_id,\n",
    "# intial_state = decoder_initial_state,\n",
    "# beam_width = 10,\n",
    "# output_layer = projection_layer,\n",
    "# length_penalty_weight = 0.0,\n",
    "# )\n",
    "\n",
    "# outputs_3, final_context_state_infer_3, _= tf.contrib.seq2seq.dynamic_decode(\n",
    "#     decoder_3,\n",
    "#     maximum_iterations=max_dec_len,\n",
    "#     output_time_major=True)\n",
    "\n",
    "# translations = outputs_3.sample_id\n",
    "\n",
    "# logits_3 = outputs_3.rnn_output\n",
    "\n",
    "# #Adding Padding to the logits\n",
    "# logit_shape_3 = tf.shape(logits_3)\n",
    "# rem_3 = max_dec_len - logit_shape_3[0]\n",
    "# paddings_3 = [[0, rem_3],[0,0],[0,0]]\n",
    "# logits_infer_3 = tf.pad(logits_3,paddings_3,'CONSTANT')\n",
    "\n",
    "# # Calculating the cross Entropy\n",
    "# crossent_3 = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=decoder_outputs_3,logits = logits_infer_3)\n",
    "# #Calculating the loss for greedy Helper type\n",
    "# loss_3 = (tf.reduce_sum(crossent_3*target_weights)/batchSize)\n",
    "\n",
    "\n",
    "\n",
    "####\n",
    "\n",
    "\n",
    "\n",
    "train_loss = loss_1#tf.cond(choice,lambda: loss_1,lambda: loss_2)\n",
    "val_loss = loss_2\n",
    "\n",
    "#Calculate and Clip Gradient \n",
    "params = tf.trainable_variables()\n",
    "gradients = tf.gradients(train_loss,params)\n",
    "clipped_gradients,_ = tf.clip_by_global_norm(gradients,max_gradient_norm)\n",
    "\n",
    "#Optimization\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "update_step = optimizer.apply_gradients(zip(clipped_gradients,params))\n",
    "\n",
    "################################ For loss_2 ############\n",
    "gradient_2 = tf.gradients(val_loss,params)\n",
    "clipped_gradient_2,_ = tf.clip_by_global_norm(gradient_2,max_gradient_norm)\n",
    "\n",
    "#Optimization\n",
    "\n",
    "optimizer_2 = tf.train.AdamOptimizer(learning_rate)\n",
    "update_step_2 = optimizer_2.apply_gradients(zip(clipped_gradient_2,params))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "encoder_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initializing the Session\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0  Started \n",
      "\n",
      "1  Started \n",
      "\n",
      "2  Started \n",
      "\n",
      "3  Started \n",
      "\n",
      "4  Started \n",
      "\n",
      "5  Started \n",
      "\n",
      "6  Started \n",
      "\n",
      "7  Started \n",
      "\n",
      "8  Started \n",
      "\n",
      "9  Started \n",
      "\n",
      "10  Started \n",
      "\n",
      "11  Started \n",
      "\n",
      "12  Started \n",
      "\n",
      "13  Started \n",
      "\n",
      "14  Started \n",
      "\n",
      "15  Started \n",
      "\n",
      "16  Started \n",
      "\n",
      "17  Started \n",
      "\n",
      "18  Started \n",
      "\n",
      "19  Started \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sp/anaconda3/lib/python3.6/site-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
      "Corpus/Sentence contains 0 counts of 4-gram overlaps.\n",
      "BLEU scores might be undesirable; use SmoothingFunction().\n",
      "  warnings.warn(_msg)\n",
      "/home/sp/anaconda3/lib/python3.6/site-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
      "Corpus/Sentence contains 0 counts of 3-gram overlaps.\n",
      "BLEU scores might be undesirable; use SmoothingFunction().\n",
      "  warnings.warn(_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch  0  Training Loss : 1.7196739196777344  Validation Loss : 189.52034072875978 , belu :  0.05010300622958298\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "0  Started \n",
      "\n",
      "1  Started \n",
      "\n",
      "2  Started \n",
      "\n",
      "3  Started \n",
      "\n",
      "4  Started \n",
      "\n",
      "5  Started \n",
      "\n",
      "6  Started \n",
      "\n",
      "7  Started \n",
      "\n",
      "8  Started \n",
      "\n",
      "9  Started \n",
      "\n",
      "10  Started \n",
      "\n",
      "11  Started \n",
      "\n",
      "12  Started \n",
      "\n",
      "13  Started \n",
      "\n",
      "14  Started \n",
      "\n",
      "15  Started \n",
      "\n",
      "16  Started \n",
      "\n",
      "17  Started \n",
      "\n",
      "18  Started \n",
      "\n",
      "19  Started \n",
      "\n",
      "epoch  0  Training Loss : 128.82231964111327  Validation Loss : 132.6122787475586 , belu :  0.18758962910904647\n",
      "101\n",
      "102\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-56-b2bc91043241>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;31m#             trainMet = False\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mfeedData\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDatafeeder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatchSize\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrainMet\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mstepLoss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mupdate_step\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_loss\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeedData\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m         \u001b[0mtotal_step_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mstepLoss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    903\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    904\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 905\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    906\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    907\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1135\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1136\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1137\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1138\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1139\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1353\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1354\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1355\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1356\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1357\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1359\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1360\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1361\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1362\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1363\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1338\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1339\u001b[0m           return tf_session.TF_Run(session, options, feed_dict, fetch_list,\n\u001b[0;32m-> 1340\u001b[0;31m                                    target_list, status, run_metadata)\n\u001b[0m\u001b[1;32m   1341\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1342\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "modelSavedOnce = 0\n",
    "#No of times whole data need to be seen by network\n",
    "bestScore = -1\n",
    "\n",
    "#In how many partition whole data will be seen by the network\n",
    "no_of_train_steps = train_dp_count//batchSize\n",
    "no_of_val_steps = val_dp_count//val_batch_size\n",
    "\n",
    "#Training Loop\n",
    "for i in range(epoch_1):\n",
    "    total_step_loss = 0\n",
    "    for step in range(no_of_train_steps):\n",
    "        trainMet = True\n",
    "#         if i >= switching_epoch:\n",
    "#             trainMet = False\n",
    "        feedData = Datafeeder(step,batchSize,trainMet)\n",
    "        _,stepLoss = sess.run([update_step,train_loss],feed_dict=feedData)\n",
    "        total_step_loss += stepLoss\n",
    "        print(step)\n",
    "        \n",
    "        if step % val_step_shift == 0:\n",
    "            dropval = 0\n",
    "            val_loss_total = 0.0\n",
    "            #Going for the joy ride of Validation\n",
    "            actualTextList = []\n",
    "            #First need Data feeder. Haha! I have that.\n",
    "            for valStep in range(no_of_val_steps):\n",
    "                print(valStep,\" Started \\n\")\n",
    "                # Get the validation data\n",
    "                feedValData = val_Datafeeder(valStep,batchSize)\n",
    "                # Loss 2 is for the validation, which can be further add into second part training\n",
    "                valStepLoss,genRatedText = sess.run([val_loss,translations],feed_dict=feedValData)\n",
    "                actualTextList += GenerateText(genRatedText)\n",
    "                val_loss_total += valStepLoss\n",
    "            #Every new score will be started from the zero.\n",
    "            score = 0\n",
    "            for s1,s2 in zip(val_belu_ref_text,actualTextList):\n",
    "                score += sentence_bleu(s1, s2,weights=(0.25, 0.25, 0.25, 0.25))\n",
    "            score = score/(len(val_belu_ref_text))            \n",
    "            if score > storingThreshold:\n",
    "                if score > bestScore:\n",
    "                    bestScore = score\n",
    "                    save_path = saver.save(sess, \"repo/model/savedModel.ckpt\")\n",
    "                    modelSavedOnce = 1\n",
    "            norm_train_loss = total_step_loss/val_step_shift\n",
    "            norm_val_loss = val_loss_total/no_of_val_steps\n",
    "            print(\"epoch \",i,\" Training Loss :\",norm_train_loss,\" Validation Loss :\", norm_val_loss,\", belu : \",score)\n",
    "            logFile.write(str(i) + \",\" + str(step) + \",\" + str(norm_train_loss) + \",\"+ str(norm_val_loss) + \",\"+str(score)+ \"\\n\")\n",
    "            logFile.flush()\n",
    "            total_step_loss = 0\n",
    "            val_loss_total = 0\n",
    "\n",
    "            dropval = dpFixed \n",
    "\n",
    "for i in range(epoch_1,epoch_1+epoch_2):\n",
    "    total_step_loss = 0\n",
    "    for step in range(no_of_train_steps):\n",
    "        trainMet = True\n",
    "#         if i >= switching_epoch:\n",
    "#             trainMet = False\n",
    "        feedData = Datafeeder(step,batchSize,trainMet)\n",
    "        _,stepLoss = sess.run([update_step_2,train_loss],feed_dict=feedData)\n",
    "        total_step_loss += stepLoss\n",
    "        print(step)\n",
    "        \n",
    "        if step % val_step_shift == 0:\n",
    "            dropval = 0\n",
    "            val_loss_total = 0.0\n",
    "            #Going for the joy ride of Validation\n",
    "            actualTextList = []\n",
    "            #First need Data feeder. Haha! I have that.\n",
    "            for valStep in range(no_of_val_steps):\n",
    "                print(valStep,\" Started \\n\")\n",
    "                # Get the validation data\n",
    "                feedValData = val_Datafeeder(valStep,batchSize)\n",
    "                # Loss 2 is for the validation, which can be further add into second part training\n",
    "                valStepLoss,genRatedText = sess.run([val_loss,translations],feed_dict=feedValData)\n",
    "                actualTextList += GenerateText(genRatedText)\n",
    "                val_loss_total += valStepLoss\n",
    "            #score = corpus_bleu(val_belu_ref_text, actualTextList, weights=(0.25,0.25,0.25,0.25))\n",
    "            score = 0\n",
    "            for s1,s2 in zip(val_belu_ref_text,actualTextList):\n",
    "                score += sentence_bleu(s1, s2,weights=(0.25, 0.25, 0.25, 0.25))\n",
    "            score = score/(len(val_belu_ref_text))            \n",
    "            if score > storingThreshold:\n",
    "                if score > bestScore:\n",
    "                    bestScore = score\n",
    "                    save_path = saver.save(sess, \"repo/model/savedModel.ckpt\")\n",
    "                    modelSavedOnce = 1\n",
    "            norm_train_loss = total_step_loss/val_step_shift\n",
    "            norm_val_loss = val_loss_total/no_of_val_steps\n",
    "            print(\"epoch \",i,\" Training Loss :\",norm_train_loss,\" Validation Loss :\", norm_val_loss,\", belu : \",score)\n",
    "            logFile.write(str(i) + \",\" + str(step) + \",\" + str(norm_train_loss) + \",\"+ str(norm_val_loss) + \",\"+str(score)+ \"\\n\")\n",
    "            logFile.flush()\n",
    "            #print(total_step_loss/100)\n",
    "            total_step_loss = 0\n",
    "            val_loss_total = 0\n",
    "            dropval = dpFixed\n",
    "\n",
    "if modelSavedOnce == 0:\n",
    "    save_path = saver.save(sess, \"repo/model/savedModel.ckpt\")\n",
    "            \n",
    "logFile.close()\n",
    "\n",
    "no_of_test_steps = int(np.ceil(test_dp_count/batchSize))\n",
    "\n",
    "actualTextList = []\n",
    "\n",
    "\n",
    "\n",
    "for testStep in range(no_of_test_steps):\n",
    "    print(testStep,\" Started \\n\")\n",
    "    feedValData = test_Datafeeder(testStep,batchSize)\n",
    "    genRatedText = sess.run(translations,feed_dict=feedValData)\n",
    "    actualTextList += GenerateText(genRatedText)\n",
    "\n",
    "fileText = open(\"repo/\"+ con_name +\"gen.txt\",\"w\")\n",
    "for li in range(test_dp_count):\n",
    "    fileText.write(actualTextList[li]+\"\\n\")\n",
    "fileText.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25080320\n"
     ]
    }
   ],
   "source": [
    "total_parameters = 0\n",
    "for variable in tf.trainable_variables():\n",
    "    # shape is an array of tf.Dimension\n",
    "    shape = variable.get_shape()\n",
    "    #print(shape)\n",
    "    #print(len(shape))\n",
    "    variable_parameters = 1\n",
    "    for dim in shape:\n",
    "        #print(dim)\n",
    "        variable_parameters *= dim.value\n",
    "    #print(variable_parameters)\n",
    "    total_parameters += variable_parameters\n",
    "print(total_parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
